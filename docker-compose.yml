# =============================================================================
# LLM RAG Agent - Microservices Architecture
# =============================================================================
# Services:
#   - rag-service:       Handles document loading, vector store, and retrieval (Port 8001)
#   - inference-service: Handles LLM agents and orchestration (Port 8000)
#
# Data Flow:
#   User -> inference-service -> rag-service -> Vector DB
#                             <- Retrieved Context
#        <- LLM Response
# =============================================================================

services:
  # ---------------------------------------------------------------------------
  # RAG Service - Document Retrieval
  # ---------------------------------------------------------------------------

  rag-service:

    build:
      context: .
      dockerfile: services/rag_service/Dockerfile
    image: llm-rag-service:latest
    container_name: rag-service
    ports:
      - "8001:8001"
    volumes:
      # Persist the Chroma vector database
      - chroma_data:/app/chroma_db
      # Mount PDF data (allows adding new PDFs without rebuild)
      - ./data:/app/data:ro # Read-only
    env_file:
      - .env
    environment:
      - PYTHONUNBUFFERED=1
      - REDIS_URL=redis://redis:6379
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://jaeger:4317
      - OLLAMA_BASE_URL=http://host.docker.internal:11434 # Access Ollama on host
    healthcheck:
      test: [ "CMD", "python", "-c", "import httpx; httpx.get('http://localhost:8001/health').raise_for_status()" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped
    networks:
      - llm-network
    extra_hosts:
      - "host.docker.internal:host-gateway"

  # ---------------------------------------------------------------------------
  # Inference Service - LLM Agents
  # ---------------------------------------------------------------------------
  inference-service:
    build:
      context: .
      dockerfile: services/inference_service/Dockerfile
    image: llm-inference-service:latest
    container_name: inference-service
    ports:
      - "8000:8000"
    env_file:
      - .env
    environment:
      - PYTHONUNBUFFERED=1
      - RAG_SERVICE_URL=http://rag-service:8001 # Internal Docker network URL
      - REDIS_URL=redis://redis:6379
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://jaeger:4317
      - OLLAMA_BASE_URL=http://host.docker.internal:11434 # Access Ollama on host
    depends_on:
      rag-service:
        condition: service_healthy # Wait for RAG service to be ready
    healthcheck:
      test: [ "CMD", "python", "-c", "import httpx; httpx.get('http://localhost:8000/health').raise_for_status()" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 5s
    restart: unless-stopped
    networks:
      - llm-network
    extra_hosts:
      - "host.docker.internal:host-gateway"

  # =============================================================================
  # OPTIONAL SERVICES - Uncomment to enable production observability
  # =============================================================================

  # ---------------------------------------------------------------------------
  # Redis - Caching Service
  # ---------------------------------------------------------------------------
  # Enables response caching via src/utils/cache.py
  # Set REDIS_URL=redis://redis:6379 in .env to use
  # ---------------------------------------------------------------------------
  redis:
    image: redis:7-alpine
    container_name: redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: [ "CMD", "redis-cli", "ping" ]
      interval: 10s
      timeout: 5s
      retries: 3
    restart: unless-stopped
    networks:
      - llm-network

  # ---------------------------------------------------------------------------
  # Jaeger - Distributed Tracing
  # ---------------------------------------------------------------------------
  # Enables trace visualization via src/scaling/tracing.py
  # Set TRACING_ENABLED=true in .env to use
  # Access UI at http://localhost:16686
  # ---------------------------------------------------------------------------
  jaeger:
    image: jaegertracing/all-in-one:1.52
    container_name: jaeger
    ports:
      - "16686:16686" # UI
      - "4317:4317" # OTLP gRPC
    environment:
      - COLLECTOR_OTLP_ENABLED=true
    restart: unless-stopped
    networks:
      - llm-network

  # ---------------------------------------------------------------------------
  # Prometheus - Metrics Collection
  # ---------------------------------------------------------------------------
  # Requires prometheus.yml configuration file
  # Access at http://localhost:9090
  # ---------------------------------------------------------------------------
  prometheus:
    image: prom/prometheus:v2.47.0
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    restart: unless-stopped
    networks:
      - llm-network

  # ---------------------------------------------------------------------------
  # Grafana - Metrics Visualization
  # ---------------------------------------------------------------------------
  # Depends on Prometheus
  # Access dashboard at http://localhost:3000 (admin/admin)
  # ---------------------------------------------------------------------------
  grafana:
    image: grafana/grafana:10.2.0
    container_name: grafana
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana_provisioning/datasources:/etc/grafana/provisioning/datasources:ro
      - ./grafana_provisioning/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./grafana_provisioning/dashboards:/var/lib/grafana/dashboards:ro
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    depends_on:
      - prometheus
    restart: unless-stopped
    networks:
      - llm-network

# =============================================================================
# Named Volumes (Persistent Storage)
# =============================================================================
volumes:
  chroma_data:
    driver: local
  # Uncomment when enabling optional services:
  redis_data:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local

# =============================================================================
# Networks
# =============================================================================
networks:
  llm-network:
    driver: bridge
